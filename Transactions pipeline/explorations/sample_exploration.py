# Databricks notebook source
# MAGIC %md
# MAGIC ### Example Exploratory Notebook
# MAGIC
# MAGIC Use this notebook to explore the data generated by the pipeline in your preferred programming language.
# MAGIC
# MAGIC **Note**: This notebook is not executed as part of the pipeline.

# COMMAND ----------

import sys

sys.path.append("/Workspace/Users/kapardi21@gmail.com/New Pipeline 2025-11-20 15:14")

# COMMAND ----------

# !!! Before performing any data analysis, make sure to run the pipeline to materialize the sample datasets. The tables referenced in this notebook depend on that step.

display(spark.sql("SELECT * FROM workspace.default.sample_aggregation_nov_20_1514"))

# COMMAND ----------

# MAGIC %sql
# MAGIC DESCRIBE EXTERNAL LOCATION `db_s3_external_databricks-s3-ingest-e34e8`;

# COMMAND ----------

# MAGIC %sql
# MAGIC -- List all files in that S3 path
# MAGIC LIST 's3://kapardi-kafka-transactions/transactions/';

# COMMAND ----------

from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType, LongType, ArrayType
)

schema = StructType([
    StructField("amount", DoubleType(), True),
    StructField("category", StringType(), True),
    StructField("currency", StringType(), True),
    StructField("items", ArrayType(
        StructType([
            StructField("item_id", StringType(), True),
            StructField("name", StringType(), True),
            StructField("price", DoubleType(), True),
            StructField("quantity", LongType(), True)
        ])
    ), True),
    StructField("timestamp", StringType(), True),
    StructField("transaction_id", StringType(), True),
    StructField("user", StructType([
        StructField("address", StringType(), True),
        StructField("age", LongType(), True),
        StructField("city", StringType(), True),
        StructField("country", StringType(), True),
        StructField("device_type", StringType(), True),
        StructField("email", StringType(), True),
        StructField("ip_address", StringType(), True),
        StructField("name", StringType(), True),
        StructField("payment_method", StringType(), True),
        StructField("user_id", LongType(), True)
    ]), True)
])


# COMMAND ----------

# DBTITLE 1,ime
transactions_df = spark.read.json("s3://kapardi-kafka-transactions/transactions/")
display(transactions_df)

# COMMAND ----------

transactions_df.select("transaction_id", "amount", "currency", "timestamp", "user.user_id").display()

# COMMAND ----------

